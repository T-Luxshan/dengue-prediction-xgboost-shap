{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dengue Cases Prediction using XGBoost and SHAP\n",
    "\n",
    "This notebook demonstrates the process of predicting dengue cases using the `dengue_data_with_weather.csv` dataset. The analysis includes data preprocessing, model training with XGBoost, evaluation, and model interpretation using SHAP values.\n",
    "\n",
    "**Note:** Please refer to the `ML_Assignment_Guidelines.pdf` for specific instructions on evaluation metrics and split criteria if they differ from standard practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if not already installed\n",
    "!pip install xgboost shap pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dengue_data_with_weather_data.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "display(df.head())\n",
    "\n",
    "# Display dataset info\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Handling missing values and encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where target variable 'Cases' is missing\n",
    "df = df.dropna(subset=['Cases'])\n",
    "\n",
    "# Handle missing weather data\n",
    "# Strategy: Fill with median of the column or interpolate if time-series continuity is assumed per district\n",
    "# Here we use interpolation for a smoother fill if data is ordered by time, otherwise median\n",
    "df['Temp_avg'] = df['Temp_avg'].interpolate(method='linear')\n",
    "df['Precipitation_avg'] = df['Precipitation_avg'].interpolate(method='linear')\n",
    "df['Humidity_avg'] = df['Humidity_avg'].interpolate(method='linear')\n",
    "\n",
    "# Alternatively, fill remaining NaNs with column median\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Check if any missing values remain\n",
    "print(\"Missing values after handling:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Categorical Variables\n",
    "# 'Province' and 'District' are categorical. We use Label Encoding for tree-based models like XGBoost.\n",
    "\n",
    "le_province = LabelEncoder()\n",
    "df['Province_Encoded'] = le_province.fit_transform(df['Province'])\n",
    "\n",
    "le_district = LabelEncoder()\n",
    "df['District_Encoded'] = le_district.fit_transform(df['District'])\n",
    "\n",
    "# Ensure 'Year' and 'Month' are integers\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['Month'] = df['Month'].astype(int)\n",
    "\n",
    "print(\"Data Types:\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Cases\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Cases'], bins=30, kde=True)\n",
    "plt.title('Distribution of Dengue Cases')\n",
    "plt.xlabel('Cases')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Correlation Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n",
    "\n",
    "Given the temporal nature of the data (Years 2019-2021), we should ideally split by time to prevent data leakage.\n",
    "We will use 2019 and 2020 for training, and 2021 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features and Target\n",
    "features = ['Year', 'Month', 'Province_Encoded', 'District_Encoded', \n",
    "            'Latitude', 'Longitude', 'Elevation', \n",
    "            'Temp_avg', 'Precipitation_avg', 'Humidity_avg']\n",
    "target = 'Cases'\n",
    "\n",
    "# Split based on Year\n",
    "train_data = df[df['Year'] < 2021]\n",
    "test_data = df[df['Year'] == 2021]\n",
    "\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training (XGBoost)\n",
    "\n",
    "We use XGBoost Regressor to predict the number of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost Regressor\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', \n",
    "                         n_estimators=100, \n",
    "                         learning_rate=0.1, \n",
    "                         max_depth=5, \n",
    "                         random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Evaluate the model using RMSE and MAE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# --- Regression Metrics ---\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"--- Regression Metrics ---\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared Score: {r2:.2f}\\n\")\n",
    "\n",
    "# --- Classification Metrics (High Case Outbreak Detection) ---\n",
    "# Defining an outbreak threshold (e.g., 75th percentile of training data)\n",
    "threshold = y_train.quantile(0.75)\n",
    "print(f\"Outbreak Threshold (75th percentile): {threshold:.2f} cases\")\n",
    "\n",
    "y_test_binary = (y_test > threshold).astype(int)\n",
    "y_pred_binary = (y_pred > threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "auc = roc_auc_score(y_test_binary, y_pred)\n",
    "\n",
    "print(\"--- Classification Metrics ---\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "print(f\"AUC Score: {auc:.2f}\")\n",
    "\n",
    "# Visualize Results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Actual vs Predicted Scatter Plot\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5, ax=axes[0])\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Cases')\n",
    "axes[0].set_ylabel('Predicted Cases')\n",
    "axes[0].set_title('Actual vs Predicted Dengue Cases')\n",
    "\n",
    "# 2. Confusion Matrix for Classification\n",
    "cm = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted Outbreak')\n",
    "axes[1].set_ylabel('Actual Outbreak')\n",
    "axes[1].set_title('Confusion Matrix (Outbreak vs No Outbreak)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation (SHAP)\n",
    "\n",
    "Using SHAP (SHapley Additive exPlanations) to understand feature importance and model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP explainer\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary Plot (Bee swarm)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Feature Importance Bar Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence Plot for a key feature (e.g., Temp_avg)\n",
    "# This shows how a specific feature interacts with the model's output\n",
    "shap.dependence_plot(\"Temp_avg\", shap_values.values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Prediction Explanation\n",
    "print(\"--- Explaining Individual Predictions ---\\n\")\n",
    "\n",
    "# 1. Select a few interesting samples (e.g., top 3 predicted cases and 2 random samples)\n",
    "top_indices = np.argsort(y_pred)[-3:][::-1]\n",
    "random_indices = np.random.choice(range(len(X_test)), 2, replace=False)\n",
    "sample_indices = list(top_indices) + list(random_indices)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    actual = y_test.iloc[idx]\n",
    "    predicted = y_pred[idx]\n",
    "    district = le_district.inverse_transform([X_test.iloc[idx]['District_Encoded']])[0]\n",
    "    month = X_test.iloc[idx]['Month']\n",
    "    \n",
    "    print(f\"--- Sample (Index {idx}) in {district} (Month: {month}) ---\")\n",
    "    print(f\"Actual Cases: {actual}, Predicted Cases: {predicted:.2f}\")\n",
    "    \n",
    "    # Get SHAP values for this instance\n",
    "    instance_shap = shap_values[idx]\n",
    "    \n",
    "    # Display Waterfall Plot for visual explanation\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    shap.waterfall_plot(instance_shap)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate Text Explanation\n",
    "    # Sort features by contribution\n",
    "    contributions = []\n",
    "    for i, feature in enumerate(features):\n",
    "        val = instance_shap.values[i]\n",
    "        contributions.append((feature, val))\n",
    "    \n",
    "    # Sort by absolute contribution strength\n",
    "    contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    explanation = f\"Explaining Prediction: The model predicted {predicted:.2f} cases. \"\n",
    "    top_pos = [f\"{f} (+{v:.2f})\" for f, v in contributions if v > 0][:2]\n",
    "    top_neg = [f\"{f} ({v:.2f})\" for f, v in contributions if v < 0][:2]\n",
    "    \n",
    "    if top_pos:\n",
    "        explanation += f\"The main factors INCREASING the prediction were: {', '.join(top_pos)}. \"\n",
    "    if top_neg:\n",
    "        explanation += f\"The main factors DECREASING the prediction were: {', '.join(top_neg)}.\"\n",
    "    \n",
    "    print(f\"Reasoning: {explanation}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}